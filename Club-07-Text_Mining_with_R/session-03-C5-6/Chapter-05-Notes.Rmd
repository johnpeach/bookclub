---
title: "Chapter 5"
author: "John Peach"
date: "3/30/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      cache = TRUE)
library(dplyr)
library(tidytext)
```

```{r}
library(tm)
data("AssociatedPress", package = 'topicmodels')
AssociatedPress
```

```{r}
terms <- Terms(AssociatedPress)
head(terms)
```
```{r}
library(dplyr)
library(tidytext)

ap_td <- tidy(AssociatedPress)
ap_td
```
```{r}
ap_sentiments <- ap_td %>% 
  inner_join(get_sentiments('bing'), by = c(term = 'word'))
ap_sentiments
```
```{r}
library(ggplot2)

ap_sentiments %>% 
  count(sentiment, term, wt = count) %>% 
  ungroup() %>% 
  dplyr::filter(n >= 200) %>% 
  mutate(n = ifelse(sentiment == 'negative', -n, n)) %>% 
  mutate(term = reorder(term, n)) %>% 
  ggplot(aes(term, n, fill = sentiment)) +
    geom_bar(stat = 'identity') +
    ylab('Contribution to sentiment') +
    coord_flip()
```
```{r}
library(methods)

data("data_corpus_inaugural", package = 'quanteda')
inaug_dfm <- quanteda::dfm(data_corpus_inaugural, verbose = FALSE)
inaug_dfm
```
```{r}
inaug_td <- tidy(inaug_dfm)
inaug_td
```


```{r}
inaug_tf_idf <- inaug_td %>% 
  bind_tf_idf(term, document, count) %>% 
  arrange(desc(tf_idf))

inaug_tf_idf
```
```{r}
library(tidyr)

year_term_counts <- inaug_td %>% 
  extract(document, "year", "(\\d+)", convert = TRUE) %>% 
  complete(year, term, fill = list(count = 0)) %>% 
  group_by(year) %>% 
  mutate(year_total = sum(count))

year_term_counts
```
```{r}
library(ggplot2)
year_term_counts %>% 
  dplyr::filter(term %in% c("god", "america", "foreign", "union",
                            "constitution", "freedom")) %>% 
  ggplot(aes(year, count / year_total)) +
    geom_point() +
    geom_smooth() +
    facet_wrap(~term, scales = "free_y") +
    scale_y_continuous(labels = scales::percent_format()) +
    ylab("% frequency of word in inaugural address")
```
```{r}
ap_td %>% 
  cast_dtm(document, term, count)
```
```{r}
library(Matrix)
m <- ap_td %>% 
  cast_sparse(document, term, count)

class(m)
```

```{r}
dim(m)
```
```{r}
library(janeaustenr)

austen_dtm <- austen_books() %>% 
  unnest_tokens(word, text) %>% 
  count(book, word) %>% 
  cast_dtm(book, word, n)

austen_dtm
```
```{r}
library(tm)
data("acq")
acq
```
```{r}
acq_td <- tidy(acq)
acq_td
```
```{r}
acq_tokens <- acq_td %>% 
  select(-places) %>% 
  unnest_tokens(word, text) %>% 
  anti_join(stop_words, by = "word")

acq_tokens %>% 
  count(word, sort = TRUE)
```

```{r}
acq_tokens %>% 
  count(id, word) %>% 
  bind_tf_idf(word, id, n) %>% 
  arrange(desc(tf_idf))
```

```{r}
library(tm.plugin.webmining)
library(purrr)
library(tidyverse)

company <- c("Microsoft", "Apple", "Google", "Amazon", "Facebook",
             "Twitter", "IBM", "Yahoo", "Netflix")
symbol <- c("MSFT", "AAPL", "GOOG", "AMZN", "FB", "TWTR", "IBM", "YHOO", "NFLX")

# Use YahooNewsSource instead of GoogleFinanceSource
download_article <- function(symbol) {
  WebCorpus(YahooNewsSource(paste0("NASDAQ:", symbol)))
}

stock_articles <- data_frame(company = company,
                             symbol = symbol) %>% 
  mutate(corpus = map(symbol, download_article))
```
```{r, eval=FALSE}
# This code does not work. I think there is a bug in the tidy verse
stock_tokens <- stock_articles %>%
  unnest(map(corpus, tidy)) %>% 
  unnest_tokens(word, text) %>% 
  select(company, datetimestamp, word, id, heading)

stock_tokens
```
```{r, eval=FALSE}
library(stringr)

stock_tf_idf <- stock_tokens %>% 
  count(company, word) %>% 
  dplyr::filter(!str_detect(word, "\\d+")) %>% 
  bind_tf_idf(word, company, n) %>% 
  arrange(-tf_idf)
```
```{r, eval=FALSE}
stock_tokens %>% 
  anti_join(stop_words, by = "word") %>% 
  count(word, id, sort = TRUE) %>% 
  inner_join(get_sentiments('afinn'), by = "word") %>% 
  group_by(word) %>% 
  summarise(contribution = sum(n * score)) %>% 
  top_n(12, abs(contribution)) %>% 
  mutate(word = reorder(word, contribution)) %>% 
  ggplot(aes(word, contribution)) +
    geom_col() +
    coord_flip() +
    labs(y = 'Frequency of word * AFINN score')
```

```{r, eval=FALSE}
stock_tokens %>% 
  count(word) %>% 
  inner_join(get_sentiments('loughran'), by = 'word') %>% 
  group_by(sentiment) %>% 
  top_n(5, n) %>% 
  ungroup() %>% 
  mutate(word = reorder(word, n)) %>% 
  ggplot(aes(word, n)) +
    geom_col() +
    coord_flip() +
    facet_wrap(~ sentiment, scales = 'free') +
    ylab("Frequency of this word in the recent financial articles")
```
```{r, eval=FALSE}
stock_sentiment_count <- stock_tokens %>% 
  inner_join(get_sentiments('loughran'), by = 'word') %>% 
  count(sentiment, company) %>% 
  spread(sentiment, n, fill = 0)

stock_sentiment_count
```

```{r, eval=FALSE}
stock_sentiment_count %>% 
  mutate(score = (positive - negative) / (positive + negative)) %>% 
  mutate(company = reorder(company, score)) %>% 
  ggplot(ase(company, score, fill = score > 0)) +
    geom_col(show.legend = FALSE) +
    coord_flip() +
    labs(x = "Company", y = "Positivity score among 20 recent news articles")
```

